{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Compute the minimum of f(&beta;) = 3&beta;<sup>2</sup> - 5&beta; + 4\n",
    "\n",
    "\n",
    "f'(&beta;) = 2*3&beta;<sup>2 - 1</sup> - 1 * 5&beta;<sup>1 - 1</sup> + 4 * 0 <br>\n",
    "f'(&beta;) = 6&beta;- 5 <br> <br>\n",
    "In order to compute the minimum, we will find a &beta; such that f'(&beta;) = 0, that is, where the slope of the line is 0. The solution(s) for &beta; will identify the x coordinate of extrema for the function. <br> <br>\n",
    "f'(&beta;) = 6&beta;- 5 = 0 <br>\n",
    "6&beta;- = 5 <br>\n",
    "&beta; = 5/6 <br> <br>\n",
    "Therfore, the minimum value for this function is 5/6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: What are the values of W1 and β1 after the first iteration of gradient decent?\n",
    "let f(W,β) be a function with parameters W and β. Suppose you want to find argmin W,β f(W,β) <br>\n",
    "To this end you will use gradient decent with initial points given by <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Description Image](\\images\\DSCI470_H1P2_desc.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Solution Image](\\images\\DSCI470_H1P2_sol.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: In Python, code a gradient decent algorithm, and test it on the function from problem 1.\n",
    "1. Deliver your code along with a short explanation of how it works.\n",
    "2. what value of the learning parameter did you use?\n",
    "3. What was the value of your minimum?\n",
    "4. Does this value agree from what you found in Problem 1? How would you fix any discrepancies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximated minimal cost: 0.8333333333333324 | Expected (actual) minimal cost: 0.83\n"
     ]
    }
   ],
   "source": [
    "# Gradient decent algorithm for functions (hard coded) that returns the optimized weights and bias, cost history, and final cost\n",
    "def gradient_descent(learning_rate, iterations):\n",
    "    x = 0\n",
    "    for i in range(iterations):\n",
    "        x = x - learning_rate * (6 * x - 5)\n",
    "    return x\n",
    "\n",
    "# Driver program\n",
    "approximate_min_cost = gradient_descent(0.01, 1000)\n",
    "print(f\"Approximated minimal cost: {approximate_min_cost} | Expected (actual) minimal cost: 0.83\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The algorithm starts with an initial guess for x, in this case, it is 0. The x variable is what we are trying to optimize (minimize) with gradient decent. The algorithm runs for a specified number of iterations, in this case, it is 10000. Each iteration updates the value of x based on the gradient of the function. A higher iteration amount, as long as the algorithm does not jump around or pass the minimum (likely a high learning rate), will approximate the minimum more accuately, as it will continue moving towards the extrema. However, increasing the iterations comes with greater computational costs. For this case, we are using the derivative (gradient) function 6 * x - 5, which is hard coded for testing. The update rule is as follows: x = x − learning_rate * gradient. The algorithm moves x in the opposite direction of the gradient, which reduces the cost (minimizing). The learning rate controls how large the steps are that the algorithm takes. A smaller learning rate takes smaller, more precise steps, while a larger learning rate takes bigger, riskier steps. After running the loop for the specified number of iterations, the algorithm returns the optimized x value.\n",
    "\n",
    "2. I used a learning rate of 0.01, but I tested it with 0.1 and 0.001 as well, and they had similar results (identical results with a high enough iteration).\n",
    "\n",
    "3. The minimum value computed by the gradient decent algorithm was 0.8333333333333324\n",
    "\n",
    "4. The actual minimum value found in problem 1 is 5/6 = 0.833... The value approximated with gradient decent, 0.8333333333333324 agrees with the value found in problem 1 as they are approximately equal. If there was any discepancies, it could be rectified by either lowering the larning rate, increasing the iterations, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Modify your gradient decent code to identify the solution to\n",
    "![Description Image](\\images\\DSCI470_H1P4_desc.jpg)\n",
    "1. Deliver your code and final solution\n",
    "2. What value of the learning rate did you use?\n",
    "3. How might this be useful in solving a regression task?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weight and bias:\n",
      "[[0.79192565]\n",
      " [0.99615843]\n",
      " [1.15433757]\n",
      " [0.93850003]\n",
      " [1.00649965]]\n",
      "Final cost: 0.001483048928135222\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Calculate and return cost, as in, squared error for the prediction\n",
    "def get_cost(x, y, w):\n",
    "    return .5 * 1 / x.shape[0] * np.sum((x @ w - y)**2)\n",
    "\n",
    "# Gradient decent algorithm for matricies that returns the optimized weights and bias, cost history, and final cost\n",
    "def gradient_descent(x, y, w, learning_rate, iterations):\n",
    "    cost_history = []\n",
    "    # Run gradient decent for given iterations\n",
    "    for i in range(iterations):\n",
    "        gradient = (1 / len(y)) * (x.T @ ((x @ w) - y))  # Gradient for all w (including bias)\n",
    "        w = w - learning_rate * gradient  # Update weights (w includes both weights and bias)\n",
    "        \n",
    "        # Compute current cost and store as cost history\n",
    "        cost = get_cost(x, y, w)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # # Print cost every 10000 iterations for testing\n",
    "        # if (i % 10000 == 0):\n",
    "        #     print(f\"Iteration {i} : Cost {cost}\")\n",
    "    \n",
    "    return w, cost_history, cost\n",
    "\n",
    "# # Driver program 1\n",
    "# data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\" \n",
    "# df = pd.read_csv(data_url, header=None)\n",
    "\n",
    "# x = np.reshape(df[23].values, (len(df[23].values), 1))\n",
    "# y = np.reshape(df[24].values, (len(df[24].values), 1))\n",
    "\n",
    "# w = np.array([[1.02], [4.9]])  # Initial weight and bias\n",
    "# ones = np.ones([x.shape[0]])\n",
    "# ones = np.reshape(ones, (len(ones), 1))\n",
    "# x_ones = np.append(ones, x, axis=1)\n",
    "\n",
    "# # Use gradient decent algorithm\n",
    "# w_best, cost_history, final_cost = gradient_descent(x_ones, y, w, 0.001, 100000)\n",
    "\n",
    "# print(f\"Optimized weight and bias:\\n{w_best}\")\n",
    "# print(f\"Final cost: {final_cost}\")\n",
    "\n",
    "# Driver program 2: Using gradient decent to minimize matricies\n",
    "a = np.array([[1, 1, 1, 1, 1], [1, 2, 4, 8, 16], [1, 3, 9, 27, 81], [1, 4, 16, 64, 256], [1, 5, 25, 125, 625]])\n",
    "b = np.array([5, 31, 121, 341, 781]).reshape(-1, 1)  # Reshape to column vector\n",
    "w = np.zeros((a.shape[1], 1))  # Initialize weights with zeros\n",
    "\n",
    "# Use gradient decent algorithm\n",
    "w_best, cost_history, final_cost = gradient_descent(a, b, w, 0.00001, 500000)\n",
    "\n",
    "print(f\"Optimized weight and bias:\\n{w_best}\")\n",
    "print(f\"Final cost: {final_cost}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Code provided in python cell above. Output is as follows: \n",
    "Optimized weight and bias:\n",
    "[[0.79192565]\n",
    " [0.99615843]\n",
    " [1.15433757]\n",
    " [0.93850003]\n",
    " [1.00649965]]\n",
    "Final cost: 0.001483048928135222\n",
    "\n",
    "2. For this case, I used a very low learning rate of 0.00001. Previously, I tried with higher learning rates such as 0.001 and even 0.0001 but these rates were too high as they would trigger runtime errors that were a result of the gradient and weights becoming too large, causing numerical instability (NAN output). \n",
    "\n",
    "3. Linear regression and minimizing a cost function are significant to solving regression tasks. In a regression task, there is a set of input features, such as a matrix, and corresponding output values, such as a vector. The goal is to identify a relationship that predicts the output values based on new input data. Gradient descent is an iterative optimization algorithm used to minimize the cost function, which measures how well a model fits the training data. In relation, the goal of regression is to minimize this cost function, meaning that the model’s predictions are as close as possible to the actual target values. By minimizing the cost, we ensure the model not only performs well on the training data but also generalizes to make accurate predictions on unseen data. This process is central to building an effective regression model that predicts outcomes based on input features. In summary, minimizing the cost function through gradient descent is essential for fitting a regression model, ensuring it learns the best possible relationship between inputs and outputs for accurate predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Visual_Principles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
